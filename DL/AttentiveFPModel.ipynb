{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_fmy_JzppmD"
   },
   "outputs": [],
   "source": [
    "# dgl officially supports: 3.8, 3.9, 3.10, 3.11, 3.12\n",
    "# DeepChem officially supports Python 3.8 through 3.10\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_33xVbv1LPGu"
   },
   "outputs": [],
   "source": [
    "# !pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu124\n",
    "# !pip install deepchem\n",
    "# !pip install dgl -f https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html\n",
    "# !pip install dgllife\n",
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upvEu1EEvweq"
   },
   "outputs": [],
   "source": [
    "# 这里import不成功，后面的模型训练就搞不了\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPYfIJP3kNxX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import deepchem as dc\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "import itertools\n",
    "from ast import literal_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4Z7XFNVhGz4"
   },
   "outputs": [],
   "source": [
    "#GPU测试\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda:0')\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "\n",
    "\n",
    "# cmd nvidia-smi -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4TR5lv0hGz5"
   },
   "outputs": [],
   "source": [
    "#1. 数据输入和特征化\n",
    "#文件路径准备\n",
    "basePath = os.getcwd()\n",
    "resultPath = basePath+'/results/'\n",
    "training_path = basePath+'/training_data/'\n",
    "external_path = basePath+'/external_data/'\n",
    "training_list = os.listdir(training_path)\n",
    "\n",
    "\n",
    "algorithm = 'AttentiveFP'\n",
    "\n",
    "#hyperparameters setting\n",
    "graph_conv_layers_values = [[32, 32], [32, 64], [64, 64]]\n",
    "attention_hidden_size_values = [64, 128]\n",
    "dense_layer_size_value = 128\n",
    "dropout_value = 0.5\n",
    "\n",
    "#设置epoch, batch_size参数\n",
    "epoch = 50\n",
    "batch_size = 32\n",
    "\n",
    "#设置超参数组合\n",
    "all_combinations = list(itertools.product(graph_conv_layers_values, attention_hidden_size_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSNxFfpLhGz5"
   },
   "outputs": [],
   "source": [
    "# 2. 定义nested_cv函数\n",
    "# 将数据集划分成三份，进行迭代。首先两份输入inner_cv进行交叉验证，记录基于f1的最佳模型，最后一份用于在模型上验证\n",
    "def nested_cv(dataset_X, dataset_y, model, epoch):\n",
    "    # 使用 StratifiedKFold 创建分层交叉验证划分\n",
    "    outer_skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(outer_skf.split(dataset_X, dataset_y)):\n",
    "        # 创建训练集和测试集的 NumpyDataset\n",
    "        inner_cv_dataset = dc.data.NumpyDataset(X=dataset_X[train_index], y=dataset_y[train_index])\n",
    "        test_dataset = dc.data.NumpyDataset(X=dataset_X[test_index], y=dataset_y[test_index])\n",
    "        model = inner_cv(inner_cv_dataset, model, epoch)\n",
    "\n",
    "        # 在测试集上进行预测\n",
    "        y_true = test_dataset.y\n",
    "        y_pred = model.predict(test_dataset)[:,1]\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        # 计算 f1\n",
    "        f1 = f1_score(y_true, y_pred_binary)\n",
    "\n",
    "        # 保存结果\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    #获取三次的均值\n",
    "    average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    return average_f1_score\n",
    "\n",
    "#2.1 定义内层循环\n",
    "#每一组超参都嵌套跑，选择表现最好的模型\n",
    "def inner_cv(inner_cv_dataset, model, epoch):\n",
    "    inner_skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    X,y = inner_cv_dataset.X, inner_cv_dataset.y\n",
    "\n",
    "    best_models = []\n",
    "    for fold, (train_index, val_index) in enumerate(inner_skf.split(X, y)):\n",
    "        # 创建训练集和测试集的 NumpyDataset\n",
    "        train_dataset = dc.data.NumpyDataset(X=X[train_index], y=y[train_index])\n",
    "        val_dataset = dc.data.NumpyDataset(X=X[val_index], y=y[val_index])\n",
    "\n",
    "\n",
    "        #模型训练\n",
    "        loss = model.fit(train_dataset, nb_epoch=epoch)\n",
    "\n",
    "        # 在测试集上进行预测\n",
    "        y_true = val_dataset.y\n",
    "        y_pred = model.predict(val_dataset)[:,1]\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "  \n",
    "        # 计算f1\n",
    "        f1 = f1_score(y_true, y_pred_binary)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # 保存全部模型\n",
    "        best_models.append(copy.deepcopy(model))  # 使用深层复制保存当前模型\n",
    "\n",
    "        # 模型初始化：对模型的每个层应用初始化\n",
    "        for module in model.model.model.modules():\n",
    "            if isinstance(module, (torch.nn.Linear, torch.nn.Conv2d)):\n",
    "                weights_init(module)\n",
    "\n",
    "    # 在best_models列表中选择在验证集上性能最好的模型\n",
    "    best_model_index = np.argmax(f1_scores)\n",
    "    best_model = best_models[best_model_index]\n",
    "    return best_model\n",
    "\n",
    "# 2.1.1 初始化权重和偏置\n",
    "def weights_init(m):\n",
    "    if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5WjFFnnhGz6"
   },
   "outputs": [],
   "source": [
    "for traindataset in training_list:\n",
    "\n",
    "    export_file_path = resultPath+'/'+algorithm+'_'+traindataset+'_f1mean.csv'\n",
    "    if not os.path.exists(export_file_path):\n",
    "\n",
    "        tar_ids = []\n",
    "        final_params = []\n",
    "        final_scores = []\n",
    "\n",
    "        pertarget_files = training_path+'/'+traindataset\n",
    "        files_list = os.listdir(pertarget_files)\n",
    "        # print(traindataset)\n",
    "        for tar_id in tqdm(files_list):\n",
    "            smiles = pd.read_csv(training_path + '/' +traindataset+'/'+tar_id, header=0,index_col=False)['c_smiles'].tolist()\n",
    "            labels = pd.read_csv(training_path + '/' +traindataset+'/'+tar_id, header=0,index_col=False)['active_label'].tolist()\n",
    "            labels = np.array(labels).reshape((len(labels), 1))\n",
    "\n",
    "            # AttentiveFPModel需要使用MolGraphConvFeaturizer进行特征提取\n",
    "            featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "            X = featurizer.featurize(smiles)\n",
    "            dataset = dc.data.NumpyDataset(X=X, y=labels)\n",
    "\n",
    "            dataset_X = dataset.X\n",
    "            dataset_y = dataset.y\n",
    "\n",
    "\n",
    "            f1_params = []\n",
    "            f1_score1 = []\n",
    "\n",
    "            for graph_conv_layers, attention_hidden_size in all_combinations:\n",
    "                # 创建AttentiveFPModel\n",
    "                model = dc.models.AttentiveFPModel(\n",
    "                    batch_size=batch_size,\n",
    "                    learning_rate=0.001,\n",
    "                    n_tasks=1,\n",
    "                    mode='classification',\n",
    "                    graph_conv_layers=graph_conv_layers,\n",
    "                    attention_hidden_size=attention_hidden_size,\n",
    "                    dense_layer_size=dense_layer_size_value,\n",
    "                    dropout=dropout_value,\n",
    "                    model_dir=\"./tmp\"\n",
    "                )\n",
    "                #AttentiveFPModel的默认优化器（optimizer）是Adam（Adaptive Moment Estimation），l;earning_rate传入adam\n",
    "                #而默认的损失函数（loss function）取决于任务的类型。对于分类任务 (mode='classification')，\n",
    "                #默认使用的是交叉熵损失函数（cross entropy loss）。这些是DeepChem中AttentiveFPModel的默认设置\n",
    "\n",
    "                model.model.to(device)\n",
    "                #molAttentiveFPModel\n",
    "\n",
    "                average_f1_score = nested_cv(dataset_X, dataset_y, model, epoch)\n",
    "\n",
    "                #记录此时的超参数\n",
    "                params = {\n",
    "                'graph_conv_layers': graph_conv_layers,\n",
    "                'attention_hidden_size': attention_hidden_size,\n",
    "                'dense_layer_size_value': dense_layer_size_value,\n",
    "                'dropout_value': dropout_value\n",
    "                }\n",
    "\n",
    "                #记录超参数，记录分数\n",
    "                f1_params.append(params)\n",
    "                f1_score1.append(average_f1_score)\n",
    "\n",
    "            #获取最佳分数以及最佳参数\n",
    "            tar_ids.append(tar_id)\n",
    "            final_params.append(f1_params[np.argmax(f1_score1)])\n",
    "            final_scores.append(max(f1_score1))\n",
    "\n",
    "        #数据输出\n",
    "        data={'targets':tar_ids, 'best_params':final_params, 'f1_score':final_scores}\n",
    "        f1_data = pd.DataFrame(data)\n",
    "        f1_data.to_csv(export_file_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ext(dataset_X, dataset_y, model, epoch, test_X, test_y,traindataset,tar_id): \n",
    "\n",
    "    train_dataset = dc.data.NumpyDataset(X=dataset_X, y=dataset_y)\n",
    "    test_dataset = dc.data.NumpyDataset(X=test_X, y=test_y)\n",
    "\n",
    "    loss = model.fit(train_dataset, nb_epoch=epoch)\n",
    "\n",
    "    y_true = test_dataset.y\n",
    "    y_pred = model.predict(test_dataset)[:,1]\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred_binary, zero_division=0) \n",
    "\n",
    "\n",
    "    # torch.save(model, external_path+algorithm+'_'+traindataset+'_'+tar_id+\".pt\")\n",
    "\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置epoch, batch_size参数\n",
    "epoch = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.DataFrame()\n",
    "\n",
    "for traindataset in training_list:\n",
    "\n",
    "    params_data = pd.read_csv(resultPath+algorithm+'_'+traindataset+'_f1mean.csv')\n",
    "    params_data.insert(0, 'dataset', traindataset)\n",
    "    \n",
    "    target_list = os.listdir(external_path+'ex_'+traindataset)\n",
    "    target_list = [item[:-4] for item in target_list]\n",
    "    # print(traindataset)\n",
    "    \n",
    "    for tar_id in tqdm(target_list):\n",
    "\n",
    "        params = literal_eval(params_data.at[target_list.index(tar_id), 'best_params'])\n",
    "        featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "\n",
    "        # 加载训练集\n",
    "        smiles = pd.read_csv(training_path +traindataset+'/'+tar_id+'.csv', header=0,index_col=False)['c_smiles'].tolist()\n",
    "        labels = pd.read_csv(training_path +traindataset+'/'+tar_id+'.csv', header=0,index_col=False)['active_label'].tolist()\n",
    "        labels = np.array(labels).reshape((len(labels), 1))\n",
    "        X = featurizer.featurize(smiles)\n",
    "        dataset = dc.data.NumpyDataset(X=X, y=labels)\n",
    "        dataset_X = dataset.X\n",
    "        dataset_y = dataset.y\n",
    "\n",
    "        # 加载外部验证集\n",
    "        smiles = pd.read_csv(external_path +'ex_'+traindataset+'/'+tar_id+'.csv', header=0,index_col=False)['c_smiles'].tolist()\n",
    "        labels = pd.read_csv(external_path +'ex_'+traindataset+'/'+tar_id+'.csv', header=0,index_col=False)['active_label'].tolist()\n",
    "        labels = np.array(labels).reshape((len(labels), 1))\n",
    "        X = featurizer.featurize(smiles)\n",
    "        dataset = dc.data.NumpyDataset(X=X, y=labels)\n",
    "        test_X = dataset.X\n",
    "        test_y = dataset.y\n",
    "        \n",
    "\n",
    "        model = dc.models.AttentiveFPModel(\n",
    "            batch_size=batch_size, \n",
    "            learning_rate=0.001,\n",
    "            n_tasks=1,\n",
    "            mode='classification',\n",
    "            graph_conv_layers=params['graph_conv_layers'],\n",
    "            attention_hidden_size=params['attention_hidden_size'],\n",
    "            dense_layer_size=params['dense_layer_size_value'],\n",
    "            dropout=params['dropout_value'],\n",
    "            )\n",
    "            \n",
    "        model.model.to(device)\n",
    "        \n",
    "        try:\n",
    "            f1_external= run_ext(dataset_X, dataset_y, model, epoch, test_X, test_y,traindataset,tar_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            f1_external = e\n",
    "            print(e)\n",
    "\n",
    "        params_data.loc[params_data['targets'] == (tar_id+'.csv'), 'external_f1'] = f1_external\n",
    "\n",
    "    data_all = pd.concat([data_all, params_data], ignore_index=True)\n",
    "    \n",
    "data_all.dropna(axis=0, how='any', inplace=True)\n",
    "data_all.to_csv(external_path+algorithm+'_ex_f1.csv',index=False)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
