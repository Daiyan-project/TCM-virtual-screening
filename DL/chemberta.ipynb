{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHnU9zMOeFgx"
   },
   "outputs": [],
   "source": [
    "!pip install simpletransformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_87HaijirG_"
   },
   "outputs": [],
   "source": [
    "# 在当前文件夹下下载chemberta预训练模型\n",
    "!git clone https://github.com/seyonechithrananda/bert-loves-chemistry.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0g6In0ajJll"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "\n",
    "# import molnet loaders from deepchem\n",
    "import sklearn\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GVX1601maLc"
   },
   "outputs": [],
   "source": [
    "#1. 路径设置\n",
    "\n",
    "\n",
    "basePath = os.getcwd()\n",
    "training_data_path = basePath+'/training_data/'\n",
    "external_path = basePath+'/external_data/'\n",
    "resultpath = basePath+'/results/'\n",
    "training_list = os.listdir(training_data_path)\n",
    "\n",
    "algorithm='Chemberta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5LQYk2nnZiz"
   },
   "outputs": [],
   "source": [
    "def nested_cv(df):\n",
    "    #模型设置与重置\n",
    "    # 设立fine_tune模型\n",
    "    # 模型将自动在GPU上运行\n",
    "    model = ClassificationModel('roberta',\n",
    "                            'seyonec/PubChem10M_SMILES_BPE_396_250',\n",
    "                            args={'evaluate_each_epoch': True,\n",
    "                                  'evaluate_during_training_verbose': True,\n",
    "                                  'no_save': True, #是否不保存，若设置为false则保存，一个epoch有接近一个G\n",
    "                                  'num_train_epochs': 10,\n",
    "                                  'auto_weights': True}) # You can set class weights by using the optional weight argument\n",
    "\n",
    "\n",
    "    outer_skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    X, y = df.smiles, df.labels\n",
    "\n",
    "    #交叉验证\n",
    "    f1_score = []\n",
    "    for train_index, val_index in outer_skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[val_index]\n",
    "        y_train, y_test = y[train_index], y[val_index]\n",
    "        train_df = pd.concat([X_train, y_train], axis=1, keys=['smiles', 'labels'])\n",
    "        val_df = pd.concat([X_test, y_test], axis=1, keys=['smiles', 'labels'])\n",
    "\n",
    "\n",
    "        # Train the model\n",
    "        model.train_model(train_df, eval_df=val_df,\n",
    "                        output_dir=os.getcwd() + '/BPE_PubChem_10M_TCM_run',\n",
    "                        args={'overwrite_output_dir': True})\n",
    "\n",
    "        # 获得f1分数\n",
    "        result, model_outputs, wrong_predictions = model.eval_model(val_df,\n",
    "                                                                    acc=sklearn.metrics.f1_score)\n",
    "\n",
    "        print(result['f1_score'])\n",
    "        f1_score.append(result['f1_score'])\n",
    "\n",
    "    average_f1_score = np.mean(f1_score)\n",
    "    return model, average_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "IaDeYx01ncdb"
   },
   "outputs": [],
   "source": [
    "#设置循环与训练\n",
    "for traindataset in training_list:\n",
    "\n",
    "    targets=[]\n",
    "    training_f1_scores=[]\n",
    "    ex_target=[]\n",
    "    external_f1_scores=[]\n",
    "\n",
    "    pertarget_files = external_path+'/ex_'+traindataset\n",
    "    files_list = os.listdir(pertarget_files)\n",
    "\n",
    "\n",
    "    df_prob_all=pd.DataFrame()\n",
    "    for tar_id in files_list:\n",
    "        smiles = pd.read_csv(training_data_path + '/' +traindataset+'/'+tar_id, header=0,index_col=False)['c_smiles'].tolist()\n",
    "        labels = pd.read_csv(training_data_path + '/' +traindataset+'/'+tar_id, header=0,index_col=False)['active_label'].tolist()\n",
    "        df = pd.DataFrame(list(zip(smiles, labels)), columns=[\"smiles\", \"labels\"])\n",
    "        #训练集\n",
    "        model, average_f1_score = nested_cv(df)\n",
    "\n",
    "        targets.append(tar_id)\n",
    "        training_f1_scores.append(average_f1_score)\n",
    "        print('targets:',targets)\n",
    "        print('trainscore:',training_f1_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #测试集\n",
    "        ex_smiles = pd.read_csv(external_path +'/'+'ex_'+traindataset+'/'+tar_id, header=0,index_col=False)['c_smiles'].tolist()\n",
    "        ex_labels = pd.read_csv(external_path +'/'+'ex_'+traindataset+'/'+tar_id, header=0,index_col=False)['active_label'].tolist()\n",
    "        ex_df = pd.DataFrame(list(zip(ex_smiles, ex_labels)), columns=[\"smiles\", \"labels\"])\n",
    "        result, model_outputs, wrong_predictions = model.eval_model(ex_df,\n",
    "                                                                acc=sklearn.metrics.f1_score)\n",
    "\n",
    "        #增加预测概率和预测标签\n",
    "\n",
    "        # 使用 softmax 函数获取概率值\n",
    "        probability_values = np.exp(model_outputs) / np.sum(np.exp(model_outputs), axis=1, keepdims=True)\n",
    "\n",
    "        # 将概率值添加到验证数据框中\n",
    "        ex_df['pred_proba'] = probability_values[:, 1]\n",
    "\n",
    "        #获取预测标签\n",
    "        predicted_labels = np.argmax(model_outputs, axis=1)\n",
    "        #添加预测标签\n",
    "        ex_df['y_test'] = predicted_labels\n",
    "        #添加靶点名\n",
    "        ex_df['target'] = tar_id\n",
    "\n",
    "\n",
    "        ex_target.append(tar_id)\n",
    "        external_f1_scores.append(result['f1_score'])\n",
    "        print('extar:',ex_target)\n",
    "        print('exscore:',external_f1_scores)\n",
    "\n",
    "\n",
    "        df_prob_all = pd.concat([df_prob_all,ex_df])\n",
    "\n",
    "    #训练数据输出\n",
    "    train_data = {'targets':targets, 'best_params':\"\", 'f1_score':training_f1_scores}\n",
    "    train_f1_data = pd.DataFrame(train_data)\n",
    "    train_f1_data.to_csv(resultpath+'/'+algorithm+'_'+traindataset+'_f1mean.csv',index=False)\n",
    "    #外部测试数据输出\n",
    "    ex_data={'ex_targets':ex_target, 'best_params':\"\", 'ex_f1_score':external_f1_scores}\n",
    "    ex_f1_data = pd.DataFrame(ex_data)\n",
    "    ex_f1_data.to_csv(resultpath+'/'+'ex_'+algorithm+'_'+traindataset+'_f1mean.csv',index=False)\n",
    "    df_prob_all.to_excel(resultpath+'/'+'ex_'+algorithm+'_'+traindataset+'.xlsx',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPE7pk_GqeOd"
   },
   "outputs": [],
   "source": [
    "# 合并文件\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "files = ['ex_Chemberta_TCM_30t','ex_Chemberta_TCM_50t','ex_Chemberta_TCM_80t','ex_Chemberta_TCM_100t','ex_Chemberta_TCM2000_30t','ex_Chemberta_TCM2000_50t','ex_Chemberta_TCM2000_80t','ex_Chemberta_TCM2000_100t']\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "for file in files:\n",
    "    df = pd.read_excel(resultpath + file + '.xlsx')\n",
    "    df['dataset'] = file.split('_')[2] + '_' + file.split('_')[3]\n",
    "    df_all = pd.concat([df_all,df])\n",
    "\n",
    "df_all.to_excel('/content/gdrive/MyDrive/Colab Notebooks/ex_results/chemberta_pred_proba.xlsx',index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNKvB/s8XjFwLWbgWLIa0j8",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
